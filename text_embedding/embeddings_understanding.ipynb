{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7 (default, Mar 10 2020, 15:43:03) \n",
      "[Clang 11.0.0 (clang-1100.0.33.17)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This document outlines techniques to preprocess text and represent it using word embedding techniques to facilitate detecting similarity between chunks of text. \n",
    "\n",
    "The motivation for this is to power intelligent inventory search in e-commerce and dataset will be specific to e-commerce. \n",
    "\n",
    "## Preprocessing\n",
    "The preprocessing of text is required to ensure cleanliness and correct format of data. Commonly followed steps include:\n",
    "* Uniform case - ensuring uniformity case of text. This step could result in loss of information such as noun capitalization\n",
    "* Remove stop words - stop words such as 'is', 'the', 'are', 'a', 'an' etc are removed\n",
    "* Remove punctuation - punctuations such as '.', '!', '?', '$' etc are removed. Note: this could lead to further loss of information from the text\n",
    "* lemmatization - process of producing morphological variants of a base word or lemma e.g. democracy, democratic, democratization\n",
    "* Stemming - heuristic method of removing affixes e.g. drawing -> draw, drawings -> draw\n",
    "\n",
    "\n",
    "## Word Embeddings\n",
    "Word embeddings are a way to represent the words within a chunk of text(Document) so as to capture as much of the semantic (vocabulary and meaning) and syntax (grammatical structure). Techniques include:\n",
    "* Bag of Words(BoW)\n",
    "* TF-IDF\n",
    "* Smooth Inverse Frequency (SIF)\n",
    "* Word2Vec\n",
    "    * Continous BoW\n",
    "    * Continous Skip-gram\n",
    "    \n",
    "## Similarity\n",
    "Once representations/vectors of text is calculated, we will be using similarity functions to determine similarity\n",
    "* Cosine similarity\n",
    "* Jaccard distance\n",
    "* Euclidean distance\n",
    "* Word Mover's distance\n",
    "    \n",
    "## Methodology\n",
    "We will be applying the following steps. Techniques applied within each step may vary.\n",
    "* Text Preprocessing\n",
    "* Feature Extraction\n",
    "* Similarity Calculation\n",
    "* Threshold/Decision Function\n",
    "\n",
    "## Dataset\n",
    "We'll be using a text similarity dataset from [Kaggle](https://www.kaggle.com/rishisankineni/text-similarity?select=train.csv). Download the dataset (`train.csv` and `test.csv`) to a directory named `data`. Once you have a satisfactory model, test your model on an e-commerce [Kaggle](https://www.kaggle.com/cclark/product-item-data) dataset. Download the data to the `data` directory and rename file to `ecommerce_product_listing.csv`. You may need to update your methodology for the new dataset.\n",
    "\n",
    "## Libraries\n",
    "We will be using the following libraries and all input dataset will be processed as per the English language semantics and syntax\n",
    "### `nltk`\n",
    "Install all supporting libraries for the `nltk` module using the provided script. \n",
    "`python -m nltk.downloader all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "from unidecode import unidecode\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stop words and punctuation in the English language\n",
    "STOP_WORDS = stopwords.words('english') + list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_process(corpus):\n",
    "    '''pre processes corpus(input text) before processing'''\n",
    "    # uniform case\n",
    "    corpus = corpus.lower()\n",
    "    # transate non ascii characters to ascii\n",
    "    corpus = unidecode(corpus)\n",
    "    # tokenize words\n",
    "    corpus = word_tokenize(corpus)\n",
    "    # remove stop words\n",
    "    corpus = [i for i in corpus if i not in STOP_WORDS]\n",
    "    # lemmatize\n",
    "    for index, word in enumerate(corpus):\n",
    "        corpus[index] = lemmatizer.lemmatize(word)\n",
    "    \n",
    "        \n",
    "    return \" \".join(corpus)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi name jose live punta cana ate couple egg went swimming yesterday'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process('Hi, my name is José. I live in Puñta Caña. I ate couple of eggs and went swimming yesterday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summer dress ideal picnic long walk park bbq', 'semi-casual attire perfect work night friend']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'summer dress ideal for picnics, long walks in the park or a BBQ', \n",
    "    'semi-casual attire, perfect for both work and a night out with friends'\n",
    "]\n",
    "for index, text in enumerate(corpus):\n",
    "    corpus[index] = pre_process(text)\n",
    "    \n",
    "# create vocabulary using uni, bi and tri grams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "# fit and transform TF-IDF\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "feature_vectors = tfidf_vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
